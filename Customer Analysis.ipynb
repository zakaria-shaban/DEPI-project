{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156717f7-9e0c-46d1-95ba-0b07392aa079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c92d8-01a9-427a-88ec-67cef5cca98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages (if not already installed)\n",
    "# These packages allow for database connections, data manipulation, and visualization\n",
    "\n",
    "%pip install pyodbc\n",
    "%pip install pymssql\n",
    "%pip install pandas\n",
    "%pip install sqlalchemy\n",
    "%pip install urllib\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "!pip install pandas matplotlib seaborn sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9b87be-335a-4175-a4ab-58b277e49388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing essential libraries\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import pyodbc\n",
    "import pymssql\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score , accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "import pickle\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1705004b-dfc0-48a6-bfbc-45eaf42e109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Werehouse\n",
    "# Step 1: Set up the connection to the OLTP (Operational) Database\n",
    "# Using pyodbc to connect to the OLTP database\n",
    "conn = pyodbc.connect(\n",
    "    'Driver={SQL Server};'\n",
    "    'Server=DESKTOP-DB1EO4N;'  # Update with your actual server name\n",
    "    'Database=customers;'       # Name of your OLTP database\n",
    "    'Trusted_Connection=yes;'   # Using Windows Authentication\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd3bf82-c26d-4e65-963e-eb9e6c5e235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Test the connection with a simple query\n",
    "# Creating a cursor to execute a simple query to validate the connection\n",
    "# Create a cursor\n",
    "cursor = conn.cursor()\n",
    "# Optionally, run a simple query to test the connection\n",
    "try:\n",
    "    cursor.execute(\"SELECT 1\")  # Test query\n",
    "    row = cursor.fetchone()\n",
    "    print(\"Query result:\", row)  # Should return (1,)\n",
    "except Exception as e:\n",
    "    print(\"Error executing query:\", e)\n",
    "finally:\n",
    "    cursor.close()  # Always close the cursor after usage\n",
    "    conn.close()    # Close the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacec58b-5eaf-4164-b91c-509c3381728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create engines for both OLTP and DWH databases\n",
    "# These engines will be used to fetch data and insert it into the DWH\n",
    "oltp_engine = create_engine('mssql+pyodbc://@localhost/customers?driver=ODBC+Driver+17+for+SQL+Server&Trusted_Connection=yes')\n",
    "dwh_engine = create_engine('mssql+pyodbc://@localhost/DWH_DB?driver=ODBC+Driver+17+for+SQL+Server&Trusted_Connection=yes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a35d45-5fe6-46f0-9711-e4d145ec788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Extract data from OLTP database into pandas DataFrames\n",
    "# Fetching the tables from the OLTP system (customers, products, staff, stores, etc.)\n",
    "customers_df = pd.read_sql(\"SELECT * FROM customer\", con=oltp_engine)\n",
    "products_df = pd.read_sql(\"SELECT * FROM products\", con=oltp_engine)\n",
    "staff_df = pd.read_sql(\"SELECT * FROM staffs\", con=oltp_engine)\n",
    "stores_df = pd.read_sql(\"SELECT * FROM store\", con=oltp_engine)\n",
    "category_df = pd.read_sql(\"SELECT * FROM category\", con=oltp_engine)\n",
    "brand_df = pd.read_sql(\"SELECT * FROM brand\", con=oltp_engine)\n",
    "stocks_df = pd.read_sql(\"SELECT * FROM stocks\", con=oltp_engine)\n",
    "orders_df = pd.read_sql(\"SELECT * FROM orders\", con=oltp_engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1273317d-21e9-455e-9e01-a50ced2eedb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Perform basic transformations on the data\n",
    "# Handling missing values by filling NaNs with a placeholder in email columns\n",
    "customers_df['email'].fillna('noemail@example.com')\n",
    "staff_df['email'].fillna('noemail@example.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c027e9-19c1-48f3-8e10-005d216a02e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Select relevant columns to create dimension tables\n",
    "# Reducing the dataframes to include only the necessary columns for each dimension table\n",
    "dim_customers_df = customers_df[['customer_id', 'first_name', 'last_name', 'email', 'street', 'city', 'state', 'zip_code']]\n",
    "dim_products_df = products_df[['product_id', 'brand_id', 'category_id', 'product_name', 'model_year', 'list_price', 'list_price_percentage']]\n",
    "dim_staff_df = staff_df[['staff_id', 'first_name', 'last_name', 'email', 'store_id', 'manager_id', 'active']]\n",
    "dim_stores_df = stores_df[['store_id', 'store_name', 'phone', 'email', 'street', 'city', 'state', 'zip_code']]\n",
    "dim_category_df = category_df[['category_id', 'category_name']]\n",
    "dim_brand_df = brand_df[['brand_id', 'brand_name']]\n",
    "dim_stocks_df = stocks_df[['store_id', 'product_id', 'quantity']]\n",
    "dim_orders_df = orders_df[['order_id', 'staff_id', 'customer_id', 'store_id', 'order_status', 'order_date', 'required_date', 'shipped_date']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a71d83-b7cf-4599-a0aa-03da354097fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Load the transformed data into the Data Warehouse (DWH)\n",
    "# Inserting the data into the respective dimension tables in the DWH\n",
    "dim_category_df.to_sql('DimCategory', con=dwh_engine, if_exists='append', index=False)\n",
    "dim_brand_df.to_sql('DimBrand', con=dwh_engine, if_exists='append', index=False)\n",
    "dim_stores_df.to_sql('DimStore', con=dwh_engine, if_exists='append', index=False)\n",
    "dim_customers_df.to_sql('DimCustomer', con=dwh_engine, if_exists='append', index=False)\n",
    "dim_products_df.to_sql('DimProduct', con=dwh_engine, if_exists='append', index=False)\n",
    "dim_staff_df.to_sql('DimStaff', con=dwh_engine, if_exists='append', index=False)\n",
    "dim_stocks_df.to_sql('DimStocks', con=dwh_engine, if_exists='append', index=False)\n",
    "dim_orders_df.to_sql('DimOrders', con=dwh_engine, if_exists='append', index=False)\n",
    "\n",
    "print(\"Dimension tables loaded successfully into the Data Warehouse.\")\n",
    "stocks_df = pd.read_sql(\"SELECT * FROM stocks\", con=oltp_engine)\n",
    "dim_stocks_df = stocks_df[['store_id', 'product_id', 'quantity']]\n",
    "dim_stocks_df.to_sql('Dimstocks', con=dwh_engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d3b561-7bc4-40d3-8334-f9f9d15dc583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Handle duplicates (if applicable)\n",
    "# Example: Check for duplicate customer_id values in the DimCustomer table\n",
    "duplicate_check = dim_customers_df[dim_customers_df.duplicated(subset=['customer_id'], keep=False)]\n",
    "\n",
    "# If no duplicates, print confirmation; otherwise, print the number of duplicates found\n",
    "if duplicate_check.empty:\n",
    "    print(\"No duplicate customer_id found!\")\n",
    "else:\n",
    "    print(f\"Found {len(duplicate_check)} duplicates.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d987c1-24fc-4acf-abfc-56fc4408214b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e49704-efa0-4ebb-88c7-d200144b7731",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analsis with Python\n",
    "# Step 1: Specify the folder path where your CSV files are stored\n",
    "folder_path = \"/content/datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769f444-a718-4e28-8b76-dea221e520d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Use glob to find all CSV files in the folder and store their paths\n",
    "csv_files = glob.glob(folder_path + \"/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeff22e-8ccd-436c-aa05-797fd3eb72a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load each CSV file into a DataFrame and store them in a dictionary\n",
    "dataframes = {file.split('/')[-1]: pd.read_csv(file) for file in csv_files}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5701a490-3738-480f-ba35-136e4cb4786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Print the first few rows of each loaded DataFrame to verify the data\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"Table: {name}\")\n",
    "    print(df.head(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd9b7c4-8555-4479-96fd-333dc72de1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Print the column names of each DataFrame for a quick overview\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"{name} columns: {df.columns}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f83c3-832d-442d-acca-b32e1a9d9954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Load specific DataFrames, assuming two tables: 'customers.csv' and 'orders.csv'\n",
    "customers = dataframes['customers.csv']\n",
    "orders = dataframes['orders.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceae8008-9dfa-4e3a-a17d-7dbac5845f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Merge 'orders' with 'customers' on 'customer_id' to associate orders with customer details\n",
    "merged_df = pd.merge(orders, customers, on='customer_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c7943-cff6-499b-b17b-d5a7f029a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Display the first few rows of the merged DataFrame to check the result\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5fedde-6283-41f2-b3da-9cd466263830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Load more CSV files as DataFrames for further analysis\n",
    "products = pd.read_csv('/content/dataset/products.csv')\n",
    "brands = pd.read_csv('/content/dataset/brands.csv')\n",
    "categories = pd.read_csv('/content/dataset/categories.csv')\n",
    "order_items = pd.read_csv('/content/dataset/order_items.csv')\n",
    "orders = pd.read_csv('/content/dataset/orders.csv')\n",
    "customers = pd.read_csv('/content/dataset/customers.csv')\n",
    "stores = pd.read_csv('/content/dataset/stores.csv')\n",
    "staffs = pd.read_csv('/content/dataset/staffs.csv')\n",
    "stocks = pd.read_csv('/content/dataset/stocks.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd21482-787e-41ec-a7aa-f9dd22748b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Check the first few rows of the products and brands DataFrames\n",
    "print(products.head())  # Products DataFrame\n",
    "print(brands.head())    # Brands DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523c24a7-3df5-4128-bba0-308108409e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Merge 'products' with 'brands' using 'brand_id' to bring brand information to products\n",
    "merged_products_brands = pd.merge(products, brands, on='brand_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07319e8b-d6ca-4ffd-a58a-a07d8a6d715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Display the merged products-brands DataFrame\n",
    "print(merged_products_brands.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0613bc6a-5eec-49e3-ae0d-56852c9f8251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13: Merge 'products' with 'brands', then merge the result with 'categories'\n",
    "products_brands = pd.merge(products, brands, on='brand_id', how='inner')\n",
    "products_full = pd.merge(products_brands, categories, on='category_id', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff26b962-7d26-4e44-a396-f87eb30e885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14: Merge 'orders' with 'customers' for customer-related analysis\n",
    "orders_customers = pd.merge(orders, customers, on='customer_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f457f7e-773f-476b-b286-8fa8c0dee074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 15: Merge 'order_items' with 'products_full' to associate items with their product and brand info\n",
    "order_items_products = pd.merge(order_items, products_full, on='product_id', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c7c0c6-ff4b-4c7d-b0df-f1a1497cc827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 16: Merge 'orders_customers' with 'stores' and 'staffs' to add store and staff details\n",
    "orders_full = pd.merge(orders_customers, stores, on='store_id', how='inner')\n",
    "orders_full = pd.merge(orders_full, staffs, on='staff_id', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950f3372-daf0-409c-ab10-f7e52286f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 17: Print the final merged DataFrame for 'order_items' with product, store, and customer details\n",
    "print(order_items_products.head())  # Check final order_items merged DataFrame\n",
    "\n",
    "\n",
    "# Merge products with brands and categories\n",
    "products_full = pd.merge(products, brands, on='brand_id', how='inner')\n",
    "products_full = pd.merge(products_full, categories, on='category_id', how='inner')\n",
    "\n",
    "# Merge order_items with products_full\n",
    "order_items_products = pd.merge(order_items, products_full, on='product_id', how='inner')\n",
    "\n",
    "# Merge orders with customers and stores\n",
    "orders_customers = pd.merge(orders, customers, on='customer_id', how='inner')\n",
    "orders_full = pd.merge(orders_customers, stores, on='store_id', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde2a80a-8032-419c-8302-d02de70278a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 18: Merge 'order_items_products' with 'orders_full' to obtain the final sales data\n",
    "sales_data = pd.merge(order_items_products, orders_full, on='order_id', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e80d479-ce34-46e4-b8aa-059152b02605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 19: Verify the final merged sales data by printing the first few rows\n",
    "print(sales_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422f98bd-9b8b-497d-95fe-cb892e41ce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 20: Print the column names of the sales data to see all available fields\n",
    "print(sales_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea361f5-e3a4-458e-b60d-7038e2d6f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 21: Group sales data by 'product_name' and calculate the total quantity sold for each product\n",
    "top_products = sales_data.groupby('product_name')['quantity'].sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd95a51-163b-4050-ba26-063bf9062466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 22: Sort the top products by quantity sold in descending order and select the top 10\n",
    "top_products = top_products.sort_values(by='quantity', ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f235e0-60ec-4377-a0da-b2a92b0497e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 23: Print the top 10 best-selling products\n",
    "print(\"Top 10 Best-Selling Products:\")\n",
    "print(top_products)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05710610-d779-4836-b010-16f4407dc2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 24: Plot a bar chart for the top 10 best-selling products\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=top_products, x='quantity', y='product_name', palette='Blues_r')\n",
    "plt.title('Top 10 Best-Selling Products')\n",
    "plt.xlabel('Quantity Sold')\n",
    "plt.ylabel('Product Name')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6005784-1b45-410e-88f0-9e2733eecca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 25: Calculate total spending per order item (quantity * (list_price - discount))\n",
    "sales_data['total_spending'] = sales_data['quantity'] * (sales_data['list_price_x'] - sales_data['discount'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee259217-74c5-41c6-9491-a929db57c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 26: Group the data by customer and calculate total spending per customer\n",
    "top_customers = sales_data.groupby(['customer_id', 'first_name', 'last_name'])['total_spending'].sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40534d86-6dda-45e9-99d3-b4644fac9722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 27: Sort the top customers by total spending and select the top 10\n",
    "top_customers = top_customers.sort_values(by='total_spending', ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0804a593-0f86-403d-80e6-75279175d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 28: Print the top 10 customers by total spending\n",
    "print(\"Top 10 Customers by Total Spending:\")\n",
    "print(top_customers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e5c022-99a2-4842-86fb-83e17dfd02a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 29: Plot a bar chart for the top 10 customers by total spending\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=top_customers, x='total_spending', y='last_name', palette='Greens_r')\n",
    "plt.title('Top 10 Customers by Total Spending')\n",
    "plt.xlabel('Total Spending')\n",
    "plt.ylabel('Customer (Last Name)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522fd808-111c-486d-ab2c-2e7f7799f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 30: Group sales data by 'store_name' and calculate total revenue per store\n",
    "store_revenue = sales_data.groupby('store_name')['total_spending'].sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a65cf-abb2-4cf3-ad24-4d78584b3246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 31: Sort stores by total revenue in descending order and print the result\n",
    "store_revenue = store_revenue.sort_values(by='total_spending', ascending=False)\n",
    "print(\"Revenue by Store:\")\n",
    "print(store_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a5bc6e-808c-4671-a489-8e5b08f275ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 32: Group sales data by 'category_name' to calculate total revenue by product category\n",
    "category_revenue = sales_data.groupby('category_name')['total_spending'].sum().reset_index()\n",
    "category_revenue = category_revenue.sort_values(by='total_spending', ascending=False)\n",
    "print(\"Revenue by Product Category:\")\n",
    "print(category_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21361b1e-3de0-4ca9-9f2e-e49adad4f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 33: Group sales data by 'brand_name' to calculate total revenue for each brand\n",
    "brand_performance = sales_data.groupby('brand_name')['total_spending'].sum().reset_index()\n",
    "brand_performance = brand_performance.sort_values(by='total_spending', ascending=False)\n",
    "print(\"Brand Performance by Revenue:\")\n",
    "print(brand_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf1bfb-8662-4683-a626-84cba7f61519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 34: Convert 'order_date' to datetime and extract the month for monthly sales analysis\n",
    "sales_data['order_date'] = pd.to_datetime(sales_data['order_date'])\n",
    "sales_data['month'] = sales_data['order_date'].dt.to_period('M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4b1f40-2911-47a1-a6ca-e30d0012f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 35: Group sales data by 'month','store_name','product_name','state_x' and calculate total spending per month\n",
    "\n",
    "# Group sales data by 'month' and calculate the total spending for each month\n",
    "monthly_sales = sales_data.groupby('month')['total_spending'].sum().reset_index()\n",
    "\n",
    "# Print the monthly sales trend to inspect how sales change over time\n",
    "print(\"Monthly Sales Trend:\")\n",
    "print(monthly_sales)\n",
    "\n",
    "# Group sales data by 'store_name' to calculate the total quantity of items sold by each store\n",
    "popular_stores = sales_data.groupby('store_name')['quantity'].sum().reset_index()\n",
    "\n",
    "# Sort the stores by quantity sold in descending order and get the top 10 performing stores\n",
    "popular_stores = popular_stores.sort_values(by='quantity', ascending=False).head(10)\n",
    "\n",
    "# Print the top 10 stores by the total quantity of products sold\n",
    "print(\"Top 10 Stores by Quantity Sold:\")\n",
    "print(popular_stores)\n",
    "\n",
    "# Group sales data by 'product_name' and calculate the average discount for each product\n",
    "avg_discount = sales_data.groupby('product_name')['discount'].mean().reset_index()\n",
    "\n",
    "# Sort products by the average discount in descending order to see which products get the highest discounts\n",
    "avg_discount = avg_discount.sort_values(by='discount', ascending=False)\n",
    "\n",
    "# Print the products with the highest average discount\n",
    "print(\"Average Discount per Product:\")\n",
    "print(avg_discount)\n",
    "\n",
    "# Group sales data by 'state_x' (representing the state of the customers) and count the unique customers in each state\n",
    "customers_by_state = sales_data.groupby('state_x')['customer_id'].nunique().reset_index()\n",
    "\n",
    "# Sort the states by the number of unique customers in descending order\n",
    "customers_by_state = customers_by_state.sort_values(by='customer_id', ascending=False)\n",
    "\n",
    "# Print the number of unique customers by state\n",
    "print(\"Number of Unique Customers by State:\")\n",
    "print(customers_by_state)\n",
    "\n",
    "# Group sales data by staff (using 'staff_id', 'first_name', and 'last_name') to calculate the total revenue generated by each staff member\n",
    "staff_revenue = sales_data.groupby(['staff_id', 'first_name', 'last_name'])['total_spending'].sum().reset_index()\n",
    "\n",
    "# Sort staff members by total revenue in descending order to see the top-performing staff\n",
    "staff_revenue = staff_revenue.sort_values(by='total_spending', ascending=False)\n",
    "\n",
    "# Print the revenue generated by each staff member\n",
    "print(\"Revenue by Staff Member:\")\n",
    "print(staff_revenue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4749a629-ce64-4bd7-b1d8-d5a4daa18d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 36: Plot a line chart\n",
    "# Plotting Store Revenue\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=store_revenue, x='total_spending', y='store_name', palette='Oranges_r')\n",
    "plt.title('Revenue by Store')\n",
    "plt.xlabel('Total Revenue')\n",
    "plt.ylabel('Store Name')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Additional Step: Plotting Revenue by Product Category\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(category_revenue['total_spending'], labels=category_revenue['category_name'],\n",
    "        autopct='%1.1f%%', startangle=140, colors=sns.color_palette('pastel'))\n",
    "plt.title('Revenue by Product Category')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Additional Step: Ensure 'month' column is in datetime format\n",
    "monthly_sales['month'] = pd.to_datetime(monthly_sales['month'].astype(str), format='%Y-%m')\n",
    "\n",
    "# Additional Step: Plotting Monthly Sales Trend\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=monthly_sales, x='month', y='total_spending', marker='o', color='purple')\n",
    "plt.title('Monthly Sales Trend')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Spending')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Additional Step: Plotting Brand Performance by Revenue\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=brand_performance, x='total_spending', y='brand_name', palette='coolwarm')\n",
    "plt.title('Brand Performance by Revenue')\n",
    "plt.xlabel('Total Revenue')\n",
    "plt.ylabel('Brand Name')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Additional Step: Plotting Top 10 Stores by Quantity Sold\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=popular_stores, x='quantity', y='store_name', palette='magma')\n",
    "plt.title('Top 10 Stores by Quantity Sold')\n",
    "plt.xlabel('Total Quantity Sold')\n",
    "plt.ylabel('Store Name')\n",
    "plt.show()\n",
    "\n",
    "# Additional Step: Plotting Number of Unique Customers by State\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=customers_by_state, x='customer_id', y='state_x', palette='viridis')\n",
    "plt.title('Number of Unique Customers by State')\n",
    "plt.xlabel('Number of Customers')\n",
    "plt.ylabel('State')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905fd370-de46-439e-bed7-fb5381263d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Cluster Model\n",
    "##Load and Preprocess the Date\n",
    "# Load the sales and customer datasets from CSV files\n",
    "\n",
    "sales_data = pd.read_csv('sales_data.csv')\n",
    "customer_data = pd.read_csv('customer_data.csv')\n",
    "\n",
    "# Check the first few rows of both datasets to inspect their structure\n",
    "print(customer_data.head())\n",
    "print(sales_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a90c1c5-4772-4826-a4cc-77b5aa76db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing customer data\n",
    "\n",
    "# Fill missing values in the 'age' column with the median of the column\n",
    "customer_data['age'].fillna(customer_data['age'].median(), inplace=True)\n",
    "\n",
    "# Initialize LabelEncoder to convert categorical variables into numeric format\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding on categorical columns in the customer_data\n",
    "customer_data['customer_id'] = label_encoder.fit_transform(customer_data['customer_id'])\n",
    "customer_data['gender'] = label_encoder.fit_transform(customer_data['gender'])\n",
    "customer_data['payment_method'] = label_encoder.fit_transform(customer_data['payment_method'])\n",
    "\n",
    "# Preprocessing sales data\n",
    "\n",
    "# Convert 'invoice_date' column to datetime format to allow for date-based analysis\n",
    "sales_data['invoice_date'] = pd.to_datetime(sales_data['invoice_date'], format='%d-%m-%Y')\n",
    "\n",
    "# Extract year, month, and day from 'invoice_date' for feature creation\n",
    "sales_data['invoice_year'] = sales_data['invoice_date'].dt.year\n",
    "sales_data['invoice_month'] = sales_data['invoice_date'].dt.month\n",
    "sales_data['invoice_day'] = sales_data['invoice_date'].dt.day\n",
    "\n",
    "# Apply label encoding on categorical columns in sales_data\n",
    "sales_data['invoice_no'] = label_encoder.fit_transform(sales_data['invoice_no'])\n",
    "sales_data['customer_id'] = label_encoder.fit_transform(sales_data['customer_id'])\n",
    "sales_data['category'] = label_encoder.fit_transform(sales_data['category'])\n",
    "sales_data['shopping_mall'] = label_encoder.fit_transform(sales_data['shopping_mall'])\n",
    "\n",
    "# Drop the 'invoice_date' column since we have already extracted relevant features from it\n",
    "sales_data = sales_data.drop(columns=['invoice_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c097c8-ee58-4524-b515-e3e2b45beb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the customer and sales data using the standard scaler\n",
    "customer_data_scaled = scaler.fit_transform(customer_data)\n",
    "sales_data_scaled = scaler.fit_transform(sales_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ed1b4-5adb-49ca-b0d2-38de33f058c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Apply K-Means clustering with 2 clusters on the scaled sales data and customer data\n",
    "kmeans_sales = KMeans(n_clusters=2, random_state=42).fit(sales_data_scaled)\n",
    "kmeans_customer = KMeans(n_clusters=2, random_state=42).fit(customer_data_scaled)\n",
    "\n",
    "# Apply DBSCAN clustering on scaled sales and customer data\n",
    "dbscan_sales = DBSCAN(eps=0.5, min_samples=5).fit(sales_data_scaled)\n",
    "dbscan_customer = DBSCAN(eps=0.5, min_samples=5).fit(customer_data_scaled)\n",
    "\n",
    "# Apply Gaussian Mixture Models (GMM) clustering with 3 components\n",
    "gmm_sales = GaussianMixture(n_components=3, random_state=42).fit(sales_data_scaled)\n",
    "gmm_customer = GaussianMixture(n_components=3, random_state=42).fit(customer_data_scaled)\n",
    "\n",
    "# Save the clustering models using joblib for future use\n",
    "joblib.dump(kmeans_sales, 'kmeans_sales.pkl')\n",
    "joblib.dump(kmeans_customer, 'kmeans_customer.pkl')\n",
    "joblib.dump(dbscan_sales, 'dbscan_sales.pkl')\n",
    "joblib.dump(dbscan_customer, 'dbscan_customer.pkl')\n",
    "joblib.dump(gmm_sales, 'gmm_sales.pkl')\n",
    "joblib.dump(gmm_customer, 'gmm_customer.pkl')\n",
    "\n",
    "# Print message to confirm model saving\n",
    "print(\"Models saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2969c-0b57-410b-8dfc-f80fa28f3782",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply K-Means clustering with 2 clusters on the scaled sales data and customer data\n",
    "kmeans_sales = KMeans(n_clusters=2, random_state=42).fit(sales_data_scaled)\n",
    "kmeans_customer = KMeans(n_clusters=2, random_state=42).fit(customer_data_scaled)\n",
    "\n",
    "# Apply DBSCAN clustering on scaled sales and customer data\n",
    "dbscan_sales = DBSCAN(eps=0.5, min_samples=5).fit(sales_data_scaled)\n",
    "dbscan_customer = DBSCAN(eps=0.5, min_samples=5).fit(customer_data_scaled)\n",
    "\n",
    "# Apply Gaussian Mixture Models (GMM) clustering with 3 components\n",
    "gmm_sales = GaussianMixture(n_components=3, random_state=42).fit(sales_data_scaled)\n",
    "gmm_customer = GaussianMixture(n_components=3, random_state=42).fit(customer_data_scaled)\n",
    "\n",
    "# Save K-Means, DBSCAN, and GMM models using pickle\n",
    "with open('kmeans_sales.pkl', 'wb') as f:\n",
    "    pickle.dump(kmeans_sales, f)\n",
    "\n",
    "with open('kmeans_customer.pkl', 'wb') as f:\n",
    "    pickle.dump(kmeans_customer, f)\n",
    "\n",
    "with open('dbscan_sales.pkl', 'wb') as f:\n",
    "    pickle.dump(dbscan_sales, f)\n",
    "\n",
    "with open('dbscan_customer.pkl', 'wb') as f:\n",
    "    pickle.dump(dbscan_customer, f)\n",
    "\n",
    "with open('gmm_sales.pkl', 'wb') as f:\n",
    "    pickle.dump(gmm_sales, f)\n",
    "\n",
    "with open('gmm_customer.pkl', 'wb') as f:\n",
    "    pickle.dump(gmm_customer, f)\n",
    "\n",
    "# Confirm that the models are saved successfully\n",
    "print(\"Models saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c11558-90f3-48db-933a-707cbc6c727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate silhouette scores for K-Means clustering results\n",
    "kmeans_sales_score = silhouette_score(sales_data_scaled, kmeans_sales.labels_)\n",
    "kmeans_customer_score = silhouette_score(customer_data_scaled, kmeans_customer.labels_)\n",
    "\n",
    "# Calculate silhouette score for DBSCAN results if not all points are classified as noise\n",
    "if len(set(dbscan_sales.labels_)) > 1:\n",
    "    dbscan_sales_score = silhouette_score(sales_data_scaled, dbscan_sales.labels_)\n",
    "else:\n",
    "    dbscan_sales_score = 'Not applicable (too many points classified as noise)'\n",
    "\n",
    "if len(set(dbscan_customer.labels_)) > 1:\n",
    "    dbscan_customer_score = silhouette_score(customer_data_scaled, dbscan_customer.labels_)\n",
    "else:\n",
    "    dbscan_customer_score = 'Not applicable (too many points classified as noise)'\n",
    "\n",
    "# Calculate silhouette score for Gaussian Mixture Model clustering results\n",
    "gmm_sales_score = silhouette_score(sales_data_scaled, gmm_sales.predict(sales_data_scaled))\n",
    "gmm_customer_score = silhouette_score(customer_data_scaled, gmm_customer.predict(customer_data_scaled))\n",
    "\n",
    "# Print silhouette scores for the different clustering algorithms\n",
    "print(\"KMeans Sales Silhouette Score:\", kmeans_sales_score)\n",
    "print(\"KMeans Customer Silhouette Score:\", kmeans_customer_score)\n",
    "\n",
    "print(\"DBSCAN Sales Silhouette Score:\", dbscan_sales_score)\n",
    "print(\"DBSCAN Customer Silhouette Score:\", dbscan_customer_score)\n",
    "\n",
    "print(\"GMM Sales Silhouette Score:\", gmm_sales_score)\n",
    "print(\"GMM Customer Silhouette Score:\", gmm_customer_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fab956-cea7-4b2b-b8c8-221c8b3ed58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set up MLflow tracking URI (adjust as needed)\n",
    "mlflow.set_tracking_uri(\"http://localhost:5300\")\n",
    "\n",
    "# Start an MLflow run to log model and metrics\n",
    "mlflow.start_run()\n",
    "\n",
    "try:\n",
    "    # Log clustering model parameters\n",
    "    mlflow.log_param(\"KMeans_clusters\", 3)\n",
    "    mlflow.log_param(\"DBSCAN_eps\", 0.5)\n",
    "    mlflow.log_param(\"DBSCAN_min_samples\", 5)\n",
    "    mlflow.log_param(\"GMM_components\", 3)\n",
    "\n",
    "    # Log model accuracy as a metric\n",
    "    mlflow.log_metric(\"accuracy\", kmeans_sales_score)\n",
    "\n",
    "    # Log the KMeans sales model to MLflow\n",
    "    mlflow.sklearn.log_model(kmeans_sales, \"kmeans_sales\")\n",
    "\n",
    "    # Get the current MLflow run ID\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    print(f\"Run ID: {run_id}\")\n",
    "\n",
    "    # Register the model in MLflow model registry\n",
    "    mlflow.register_model(f\"runs:/{run_id}/kmeans_sales\", \"kmeans_sales\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle potential errors\n",
    "    print(f\"Error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # End the MLflow run\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47e7c51-37d7-41ef-84a6-d9123c0959e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the model name and model instance\n",
    "model_name = \"Customer Data Management FOR CUST AND SALES\"\n",
    "model = kmeans_sales\n",
    "\n",
    "# Initialize the MLflow client\n",
    "client = MlflowClient()\n",
    "\n",
    "# Set an MLflow experiment (create if necessary)\n",
    "mlflow.set_experiment(\"Customer Data Management \")\n",
    "\n",
    "# Log the model to MLflow under the model name\n",
    "mlflow.sklearn.log_model(model, model_name)\n",
    "\n",
    "# Get the active run details\n",
    "run = mlflow.active_run()\n",
    "\n",
    "# Retrieve and print the run ID\n",
    "run_id = run.info.run_id\n",
    "mlflow.log_metric(\"accuracy\", kmeans_sales_score)\n",
    "\n",
    "print(f\"Run ID: {run_id}\")\n",
    "print(f\"Model accuracy: {kmeans_sales_score}\")\n",
    "\n",
    "# Register the model in the MLflow model registry\n",
    "client.create_registered_model(model_name)\n",
    "client.create_model_version(model_name, f\"runs:/{run_id}/random_forest_Model_test1\", run_id)\n",
    "\n",
    "# Print confirmation of successful model registration\n",
    "print(f\"Model {model_name} registered successfully!\")\n",
    "# Initialize the MLflow client\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e81eb1-2bd0-401b-9847-8ea7465ef924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign an alias to a specific version of a registered model\n",
    "\n",
    "\n",
    "# Function to assign alias to a model version\n",
    "def assign_alias_to_version(model_name, version, alias):\n",
    "    \"\"\"\n",
    "    Assign an alias to a specific version of a registered model.\n",
    "    \n",
    "    :param model_name: The name of the registered model.\n",
    "    :param version: The version number of the model to assign the alias.\n",
    "    :param alias: The alias to assign to the model version.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "     # Check if model exists\n",
    "    try:\n",
    "        # Set the alias for the model version\n",
    "        client.set_registered_model_alias(model_name, alias, version)\n",
    "        print(f\"Alias '{alias}' assigned to version {version} of model '{model_name}'\")\n",
    "    except Exception as e:\n",
    "        # Handle potential errors during alias assignment\n",
    "         print(f\"Error occurred: {e}\")\n",
    "# Parameters for assigning an alias to a specific version of a registered model\n",
    "model_name = \"Customer Data Management FOR CUST AND SALES\"  # Name of the model to assign an alias to\n",
    "version = 2  # Specify the version number to which the alias will be assigned (ensure this version exists)\n",
    "alias = \"currentproductionmodel\"  # Specify the alias to be assigned (ensure it's a valid string)\n",
    "\n",
    "# Call the function to assign the alias to the specified version of the model\n",
    "assign_alias_to_version(model_name, version, alias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c43ff1-4890-4128-8a7a-2a5c6b5c0377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a612f4-c360-460f-afc2-d348673b89a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be49686b-81ac-4e91-a44b-6130186a88b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a110c5-7d6a-4c4e-8191-f4b60f16d8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c87f0d4-24ac-40bc-a098-a2d8fdcf854c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb2698a-9054-4e23-8fdd-5642b2359eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef27c2-5783-4d88-b70b-8a1eb967f3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e774f91-6715-4544-a070-c2996ba0550a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f36b1b2-2822-4a6f-a513-3e120088bc96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2929f2a4-d64f-4a81-9b6d-66c496b3c1b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a573e45-18da-4122-89aa-cfda9d0a1464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd71b883-b488-4aab-8d68-815f4a626d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c0fb4-cb08-417a-8aa9-03ba9ee138d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6136a7cb-70aa-4f33-9912-eb5c92246f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a07dec6-51b6-4b7e-b281-29a94324ccdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e591eb4b-2eae-4fda-be9f-1e154beb8829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee41a81-3da8-41e0-ac1e-f2e5ae68d932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ded227-e654-48f9-8e29-217582936c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebfbce1-7d64-4b9f-ab74-9b8b84292ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7879e1d-fd4a-45ec-8eab-74ab0409a24a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e456fb7d-1b75-45ca-9f62-4a4c1f40e559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dd595b-4e1d-4a22-8fbb-392653801f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44186b9-e2bf-4196-9f62-d5c1e478d8f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
